# Annotation Scripts Documentation

This directory contains scripts for the complete annotation workflow for instance segmentation using SAM3.

## Workflow Overview

The annotation workflow consists of three main steps:

1. **Generate Masks** → SAM3 Studio Frontend → COCO JSON (unlabeled)
2. **Assign Labels** → `label_interactive.py` → COCO JSON (labeled)
3. **Visualize** → `visualize_annotations.py` → Visualization PNG

### Workflow Diagram

```
Images → SAM3 Studio Frontend → COCO JSON (unlabeled) → label_interactive.py → COCO JSON (labeled) → visualize_annotations.py
```

---

## Step 1: Generate Masks with SAM3 Studio Frontend

### Purpose

The **SAM3 Studio Frontend** is the primary tool for generating instance masks. It provides an interactive web interface for precise, user-guided segmentation.

### Features

- **Interactive web interface** at `http://localhost:3000`
- Upload images and use **text prompts, box prompts, or point prompts** to guide segmentation
- Fine-tune masks interactively with positive/negative clicks
- Real-time visualization of masks and bounding boxes
- Save annotations directly from the browser

### Usage

1. **Start the frontend:**

   ```bash
   cd /path/to/sam3/app/frontend
   npm run dev
   ```
   
   **Or start both backend and frontend:**
   
   ```bash
   # Terminal 1: Backend
   cd /path/to/sam3/app/backend
   python main.py
   
   # Terminal 2: Frontend
   cd /path/to/sam3/app/frontend
   npm run dev
   ```
2. **Open in browser:**

   - Navigate to `http://localhost:3000`
3. **Generate masks:**

   - Upload an image
   - Use text prompts (e.g., "fungus", "mold")
   - Or draw boxes/click points to guide segmentation
   - Refine masks with positive/negative point clicks
   - Click "Save Annotations" button
4. **Output:**

   - Annotations are saved to `annotations/session_{timestamp}/{image_id}_annotations.json`
   - Individual mask PNGs and visualizations are also saved
   - All instances start with `category_id: null` and need to be labeled in the next step

---

## Script 1: `label_interactive.py`

### Purpose

Interactive command-line tool for manually assigning class labels to instances generated by `annotate.py`. This script allows you to view each instance and assign it to one of the predefined fungus categories.

### Role in Annotation Workflow

- **Input**: COCO JSON files with `category_id: null` (from `annotate.py`)
- **Output**: Updated COCO JSON files with assigned `category_id` values
- **What it does**:
  - Loads annotation JSON file
  - Displays each instance visualization image
  - Prompts user to select a category for each instance
  - Updates the JSON file with assigned category IDs
  - Allows skipping instances or going back to previous ones

### Usage

**Basic command:**

```bash
cd /path/to/sam3
python scripts/annotate/label_interactive.py --annotation-file annotations/{session_dir}/{image_id}_annotations.json --output-dir annotations/{session_dir}
```

**Example:**

```bash
python scripts/annotate/label_interactive.py \
  --annotation-file annotations/session_20251228_175838/2_annotations.json \
  --output-dir annotations/session_20251228_175838
```

**Interactive Process:**

1. Script loads the annotation file
2. For each instance:
   - Opens the visualization image (`{image_id}_instance_{XXX}_vis.png`)
   - Displays available categories:
     ```
     Available categories:
     1. aspergillus
     2. penicillium
     3. rhizopus
     4. mucor
     5. other_fungus
     ```
   - Prompts: `Enter category ID for instance {XXX} (or 's' to skip, 'b' to go back): `
3. User enters category ID (1-5) or command:
   - `1-5`: Assign category
   - `s`: Skip this instance (keeps `category_id: null`)
   - `b`: Go back to previous instance
   - `q`: Quit (saves progress)
4. After labeling all instances, saves updated JSON file

**Commands:**

- `--annotation-file`: Path to the COCO JSON annotation file
- `--output-dir`: Directory where the updated JSON will be saved (usually same as input directory)

**Features:**

- Automatically opens visualization images using system default viewer
- Shows current progress (e.g., "Instance 3 of 67")
- Allows going back to correct mistakes
- Saves progress incrementally
- Handles missing visualization images gracefully

**Note:** The script requires visualization images (`*_vis.png`) to be in the same directory as the annotation JSON file.

---

## Script 2: `visualize_annotations.py`

### Purpose

Creates comprehensive visualizations of annotation files, showing segmentation masks, bounding boxes, class labels, and confidence scores. Works with both unlabeled (`category_id: null`) and labeled annotations.

### Role in Annotation Workflow

- **Input**: COCO JSON annotation files (labeled or unlabeled)
- **Output**: Visualization PNG images
- **What it does**:
  - Loads annotation JSON file
  - Finds corresponding image file
  - Decodes RLE masks
  - Draws colored mask overlays for each instance
  - Draws bounding boxes
  - Adds text labels (category name or "Unlabeled")
  - Adds confidence scores
  - Saves final visualization

### Usage

**Basic command (auto-finds image):**

```bash
cd /path/to/sam3
python scripts/annotate/visualize_annotations.py annotations/{session_dir}/{image_id}_annotations.json
```

**Specify image directory:**

```bash
python scripts/annotate/visualize_annotations.py \
  annotations/session_20251228_175838/2_annotations.json \
  --image-dir assets/images/
```

**Custom output path:**

```bash
python scripts/annotate/visualize_annotations.py \
  annotations/session_20251228_175838/2_annotations.json \
  --output my_custom_visualization.png
```

**Hide scores or labels:**

```bash
# Hide scores
python scripts/annotate/visualize_annotations.py \
  annotations/session_20251228_175838/2_annotations.json \
  --no-scores

# Hide labels
python scripts/annotate/visualize_annotations.py \
  annotations/session_20251228_175838/2_annotations.json \
  --no-labels

# Hide both
python scripts/annotate/visualize_annotations.py \
  annotations/session_20251228_175838/2_annotations.json \
  --no-scores --no-labels
```

**Adjust mask transparency:**

```bash
python scripts/annotate/visualize_annotations.py \
  annotations/session_20251228_175838/2_annotations.json \
  --mask-alpha 0.3
```

**Full options:**

```bash
python scripts/annotate/visualize_annotations.py \
  --annotation-file annotations/session_20251228_175838/2_annotations.json \
  --image-dir assets/images/ \
  --output custom_output.png \
  --mask-alpha 0.5 \
  --no-scores \
  --no-labels
```

**Command-line Arguments:**

- `json_file` (required): Path to the annotation JSON file
- `--image-dir`: Directory containing images (default: auto-detect relative to JSON file)
- `--output`: Output path for visualization (default: `{json_filename}_visualization.png`)
- `--no-labels`: Hide class labels
- `--no-scores`: Hide probability scores
- `--mask-alpha`: Mask overlay transparency 0.0-1.0 (default: 0.5)

**Output:**

- Saves visualization as PNG file
- Default filename: `{json_filename}_visualization.png` in same directory as JSON
- Shows:
  - Colored mask overlays (semi-transparent)
  - Bounding boxes (colored rectangles)
  - Class labels (category name or "Unlabeled")
  - Confidence scores (if available)

**Image Auto-Detection:**
The script automatically searches for image files in these locations (relative to JSON file):

1. Same directory as JSON file
2. `../assets/images/`
3. `../../assets/images/`

**Color Coding:**

- Each instance gets a unique color
- If `category_id` is assigned, color is based on category ID
- If `category_id` is `null`, color is based on instance index
- Colors cycle through a palette of 10 distinct colors

---

## Complete Workflow Example

Here's a complete example of the annotation workflow:

### Step 1: Generate Initial Annotations with SAM3 Studio Frontend

1. Start the frontend:

   ```bash
   # Terminal 1: Backend
   cd /path/to/sam3/app/backend
   python main.py
   
   # Terminal 2: Frontend
   cd /path/to/sam3/app/frontend
   npm run dev
   ```
2. Open `http://localhost:3000` in your browser
3. Upload an image (e.g., `assets/images/2.jpg`)
4. Use prompts to segment instances:

   - **Text prompt**: Type "fungus" or "mold" in the text box
   - **Box prompt**: Draw boxes around regions to include/exclude
   - **Point prompt**: Click points to add positive/negative guidance
5. Refine masks with additional prompts as needed
6. Click "Save Annotations" button
7. Annotations are saved to `annotations/session_{timestamp}/2_annotations.json` with:

   - All instances have `category_id: null` (unlabeled)
   - Individual mask PNGs (`2_instance_XXX_mask.png`)
   - Visualization images (`2_instance_XXX_vis.png`)

### Step 2: Label Instances Interactively

```bash
python scripts/annotate/label_interactive.py \
  --annotation-file annotations/1_annotations.json \
  --output-dir annotations/
```

For each instance, you'll:

1. See the visualization image
2. Enter category ID (1-5) or skip
3. Continue until all instances are labeled

### Step 3: Visualize Results

```bash
# Visualize before labeling (shows "Unlabeled")
python scripts/annotate/visualize_annotations.py annotations/1_annotations.json

# Visualize after labeling (shows category names)
python scripts/annotate/visualize_annotations.py annotations/1_annotations.json
```

---

## File Structure

```
sam3/
├── app/
│   ├── backend/                 # FastAPI backend (PyTorch)
│   └── frontend/                # Next.js frontend (SAM3 Studio)
├── scripts/
│   └── annotate/                # Annotation workflow scripts
│       ├── label_interactive.py     # Step 2: Assign labels
│       ├── visualize_annotations.py # Step 3: Visualize results
│       └── README.md                # This file
├── assets/
│   └── images/                  # Input images (optional)
│       ├── 1.jpg
│       └── 2.jpg
└── annotations/                  # Output directory
    └── session_{timestamp}/     # Session directories
        ├── {image_id}_annotations.json
        ├── {image_id}_instance_XXX_mask.png
        ├── {image_id}_instance_XXX_vis.png
        └── {image_id}_annotations_visualization.png
```

---

## Dependencies

All scripts require:

- Python 3.10+
- PIL/Pillow
- NumPy
- `pycocotools` (for RLE encoding/decoding, optional but recommended)

Install dependencies:

```bash
cd /path/to/sam3
pip install pycocotools pillow numpy
```

Note: These scripts work with annotation JSON files and don't require the SAM3 model to be loaded. They can be run independently after annotations are generated by the frontend.

---

## Tips

1. **Image Naming**: Use numeric filenames (e.g., `1.jpg`, `2.jpg`) for consistent `image_id` assignment
2. **Batch Processing**: `annotate.py` processes all images in `assets/images/` automatically
3. **Labeling Efficiency**: Use `label_interactive.py` to label instances systematically
4. **Quality Check**: Use `visualize_annotations.py` to verify annotations before training
5. **Backup**: Keep backups of JSON files before running `label_interactive.py` (it modifies files in-place)

---

## Troubleshooting

**Issue**: `annotate.py` can't find images

- **Solution**: Ensure images are in `assets/images/` directory with numeric filenames

**Issue**: `label_interactive.py` can't open images

- **Solution**: Ensure visualization PNGs (`*_vis.png`) are in the same directory as the JSON file

**Issue**: `visualize_annotations.py` can't find image file

- **Solution**: Use `--image-dir` to specify the image directory explicitly

**Issue**: RLE decoding errors

- **Solution**: Install `pycocotools`: `uv pip install pycocotools`

---

## JSON Annotation Format

The annotation files follow the COCO (Common Objects in Context) format with additional fields. Here's the structure:

### Complete Example

```json
{
  "id": 1,
  "image_id": 2,
  "category_id": null,
  "segmentation": {
    "counts": [804394, 10, 1007, 20, 995, 30, ...],
    "size": [982, 1023]
  },
  "bbox": [270.86, 785.04, 354.05, 870.36],
  "area": 5430,
  "iscrowd": 0,
  "score": 0.782,
  "instance_id": 0
}
```

### Field Descriptions

- **`id`**: Unique annotation ID (starts at 1, increments for each instance)
- **`image_id`**: ID of the image this annotation belongs to (matches image filename without extension)
- **`category_id`**: Class label ID (null before labeling, 1-5 after labeling with `label_interactive.py`)
- **`segmentation`**: RLE (Run-Length Encoding) format for the binary mask
  - **`counts`**: Array of alternating background/foreground pixel counts (see RLE explanation below)
  - **`size`**: `[height, width]` of the mask in pixels
- **`bbox`**: Bounding box coordinates `[x0, y0, x1, y1]` in pixel coordinates
- **`area`**: Total number of pixels in the mask (foreground pixels)
- **`iscrowd`**: Always `0` (indicates single instance, not a crowd)
- **`score`**: Confidence score from SAM3 (0.0 to 1.0)
- **`instance_id`**: Zero-based index of the instance (0, 1, 2, ...)

### Understanding RLE Counts

The `segmentation.counts` array uses **Run-Length Encoding (RLE)** to efficiently store binary masks. The mask is flattened row-by-row, and the counts alternate between background and foreground pixels:

- **Even indices** (0, 2, 4...): Background pixel counts (skip these)
- **Odd indices** (1, 3, 5...): Foreground pixel counts (part of the mask)

**Example:**
```json
"counts": [804394, 10, 1007, 20, 995, 30, ...]
```

This means:
- Skip 804,394 background pixels (start of image)
- Mark 10 pixels as foreground (part of mask)
- Skip 1,007 background pixels
- Mark 20 pixels as foreground
- Skip 995 background pixels
- Mark 30 pixels as foreground
- ... and so on

The decoder uses the `size` field `[982, 1023]` to reshape the flattened array back into a 2D mask.

### Before vs After Labeling

**Before labeling** (`label_interactive.py`):
```json
{
  "category_id": null,
  ...
}
```

**After labeling**:
```json
{
  "category_id": 2,  // e.g., penicillium
  ...
}
```

---

## Notes

- **Mask Generation**: Use SAM3 Studio Frontend for interactive, precise mask generation
- All scripts use relative paths and should be run from the `sam3` directory
- The annotation format follows COCO standard with additional fields (`score`, `instance_id`)
- Categories are predefined for fungus types but can be modified in the scripts
- Visualization works with both labeled and unlabeled annotations (shows "Unlabeled" for null categories)
- Frontend saves annotations to timestamped session directories for organization
- This is a cross-platform PyTorch implementation (works on Windows, macOS, and Linux)
